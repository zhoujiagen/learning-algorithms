# 4 Syntax Analysis

## 4.1 Introduction
### 4.1.1 The Role of the Parser
### 4.1.2 Representative Grammars
### 4.1.3 Syntax Error Handling
### 4.1.4 Error-Recovery Strategies

## 4.2 Context-Free Grammars
### 4.2.1 The Formal Definition of a Context-Free Grammar

A context-free grammar consists of terminals, nonterminals, a start symbol, and productions:

1. **Terminals** are the basic symbols from which strings are formed.
2. **Nonterminals** are syntactic variables that denote sets of strings.
3. In a grammar, one nonterminal is distinguished as the **start symbol**, and the set of strings it denotes is the language generated by the grammar.
4. The **productions** of a grammar specify the manner in which the terminals and nonterminals can be combined to form strings. Each production consists of:

(4.1) a nonterminal called the *head* or *left side* of the production;

(4.2) the symbol `->`;

(4.3) a *body* or *right side* consisting of zero or more terminals and nonterminals.

### 4.2.2 Notational Conventions

(1) These symbols are terminals:

- (a) lowercase letters early in the alphabet: `a, b, c`;
- (b) operator symbols: `+, -` and so on;
- (c) punctuation symbols: `()`, `,` and so on;
- (d) the digits: `0, 1, ..., 9`;
- (e) boldface strings: **`id`**, **`if`** and so on.

(2) These symbols are nonterminals:

- (a) uppercase letters early in the alphabet: `A, B, C`;
- (b) the letter `S`: it is usually the start symbol;
- (c) lowercase, italic names: *`expr`*, *`stmt`*;
- (d) when discussing programming constructs, uppercase letter may be used to represent nonterminals for the constructs: expression `E`, terms `T`, factors `F`;

(3) uppercase letters late in the alphabet, `X, Y, Z`, represents *grammar symbols*, that is either nonterminals or terminals;

(4) lowercase letters late in the alphabet, `u, v, ..., z`, represent strings of terminals;

(5) lowercase Greek letters $\alpha$, $\beta$, $\gamma$, represent strings of grammar symbols;

(6) A set of productions $A \rightarrow \alpha_{1}, A \rightarrow \alpha_{2}, \cdots, A \rightarrow \alpha_{k}$ with a commom head `A` (A-productions), may be written $A \rightarrow \alpha_{1} | \alpha_{2} | \cdots | \alpha_{k}$;

(7) unless stated otherwise, the head of the first production is the start symbol.

### 4.2.3 Derivations

consider a nonterminal `A` in the middle of a sequence of grammar symbols, $\alpha A \beta$, where $\alpha$ and $\beta$ are arbitrary strings of grammar symbols, suppose $A \rightarrow \gamma$ is a production, then we write $\alpha A \beta \Rightarrow \alpha \gamma \beta$.

- $\Rightarrow$: derives in one step.

When a sequence of derivation steps $\alpha_{1} \Rightarrow \alpha_{2} \Rightarrow \cdots \alpha_{n}$ rewrites $\alpha_{1}$ to $\alpha_{n}$, we say $\alpha_{1}$ **derives** $\alpha_{n}$.

- $\overset{\ast}{\Rightarrow}$: derives in zero or more steps.
- $\alpha \overset{\ast}{\Rightarrow} \alpha$ for any string $\alpha$;
- if $\alpha \overset{\ast}{\Rightarrow} \beta$ and $\beta \Rightarrow \gamma$, then $\alpha \overset{\ast}{\Rightarrow} \gamma$.


If $S \overset{\ast}{\Rightarrow} \alpha$, `S` is the start symbol of a grammar G, we say that $\alpha$ is a **sentential form** of G.

A **sentence** of G is a sentential form with no nonterminals.

The **language** generated by a grammar is its set of sentences. A language that can be generate by a grammar is said to be a **context-free language**.


consider derivations in which the nonterminals to be replaced at each step:

- in **leftmost** derivations, the leftmost nonterminal in each sentential is always chosen. if $\alpha \Rightarrow \beta$ is a step in which the leftmost nonterminal in $\alpha$ is replaced, we weite $\alpha \underset{lm}{\Rightarrow} \beta$.
- in **rightmost** derivations, the rightmost nonterminal is always chosend, we write $\alpha \underset{rm}{\Rightarrow} \beta$.

$\alpha \overset{\ast}{\underset{lm}{\Rightarrow}} \beta$:  $\alpha$ derives $\beta$ by a leftmorst derivation.

If $S \overset{\ast}{\underset{lm}{\Rightarrow}} \alpha$, the we say that $\alpha$ is a **left-sentential form** of the grammar.

rightmost derivations are sometimes callled **canonical derivations**.

### 4.2.4 Parse Trees and Derivations
### 4.2.5 Ambiguity
### 4.2.6 Verifying the Language Generated by a Grammar

A proof that a grammar G generates a language L has two parts:

- show that every string generate by G is in L;
- every string in L can indeed be generated by G.

### 4.2.7 Context-Free Grammars Versus Regular Expressions

Every regular language is a context-free language, but not vice-versa.

### 4.2.8 Exercises for Section 4.2

## 4.3 Writing a Grammar
### 4.3.1 Lexical Versus Syntactic Analysis
### 4.3.2 Eliminating Ambiguity
### 4.3.3 Elimination of Left Recursion

==Algorithm 4.19: Eliminating left recursion.==

### 4.3.4 Left Factoring

==Algorithm 4.21: Left factoring a grammar.==

### 4.3.5 Non-Context-Free Language Constructs

A few syntactic constructs found in typical programming languages cannot be specified using grammars alone:

- the problem of checking that identifiers are declared before they are used in a program;
- the problem of checking that the number of formal parameters in the declaration of a function agrees with the number of actual parameters in a use of the function.

### 4.3.6 Exercises for Section 4.3

## 4.4 Top-Down Parsing
### 4.4.1 Recursive-Descent Parsing
### 4.4.2 FIRST and FOLLOW

Purpose:

- during top-down parsing, `FIRST` and `FOLLOW` allow us to choose which production to apply, based on the next input symbol;
- during panic-mode error recovery, sets of tokens produced by `FOLLOW` can be used as synchronizing tokens.

$FIRST(\alpha)$:

- $\alpha$ is any string of grammar symbols; the set of terminals that begin string derived from `\alpha`.
- If $\alpha \overset{\ast}{\Rightarrow} \epsilon$, then $\epsilon$ is also in $FIRST(\alpha)$.

$FOLLOW(A)$:

- for nonterminal `A`, the set of terminals `a` that can appear immediately to the right of `A` in some sentential form: $S \overset{\ast}{\Rightarrow} \alpha A a \beta$.
- there may have been symbols between `A` and `a` at some time during the derivation, but if so, they derive $\epsilon$ and disappeared.
- If `A` can be the rightmost symbol in some sentential form, then `$` is in $FOLLOW(A)$.

> TBD: method for calculating $FIRST(\alpha)$ and $FOLLOW(A)$

### 4.4.3 LL(1) Grammars

Predictive parsers (that is recursive-descent parsers needing no backtracking) can be constructed for a class of grammar called **LL(1)**:

- first `L`: scanning the input from left to right,
- second `L`: producing a leftmost derivation,
- `1`: using one input symbol of lookahead at each step to make parsing action decisions.

A grammar G is LL(1) *if and only if* whenever $A \rightarrow \alpha | \beta$ are two distinct productions of G, the following conditions hold:

- 1 for no terminal `a` do both $\alpha$ and $\beta$ derive strings beginning with `a`;
- 2 at most one of $\alpha$ and $\beta$ can derive the empty string;
- 3 if $\beta \overset{\ast}{\Rightarrow} \epsilon$, then $\alpha$ does not derive any string beginning with a terminal in $FOLLOW(A)$. If $\alpha \overset{\ast}{\Rightarrow} \epsilon$, then $\beta$ does not derive any string beginning with a terminal in $FOLLOW(A)$.

==Algorithm 4.31: Construction of a predictive parsing table.==

### 4.4.4 Nonrecursive Predictive Parsing

==Algorithm 4.34: Table-driven predictive parsing.==

### 4.4.5 Error Recovery in Predictive Parsing
### 4.4.6 Exercises for Section 4.4

## 4.5 Bottom-Up Parsing

A **bottom-up parse** corresponds to the construction of a parse tree for an input beginning at the leaves, and working up towards the root.

### 4.5.1 Reductions

> We can think of bottom-up parsing as the process of **reducing** a string `w` to the start symbol of the grammar.

At each reduction step, a specific substring matching the body of a production, is replaced by the nonterminal at the head of that production.

### 4.5.2 Handle Pruning
### 4.5.3 Shift-Reduce Parsing

**shift-reduce parsing** is a form of the bottom-up parsing in which a *stack* holds grammar symbols, and an *input buffer* holds the rest of the string to be parsed.

4 actions:

- (1) **shift**: shift the next input symbol onto the top of the stack;
- (2) **reduce**: the right end of the string to be reduced must be at the top of the stack; locate the left end of the string with the stack and decide with what nonterminal to replace the string;
- (3) **accept**: announce successful completion of parsing;
- (4) **error**: discover a syntax error (and call an error recovery routine).

### 4.5.4 Conflicts During Shift-Reduce Parsing

- shift/reduce conflict
- reduce/reduce conflict

### 4.5.5 Exercises for Section 4.5

## 4.6 Introduction to LR Parsing: Simple LR

**LR(k)** parsing: type of bottom-up parser

- `L`: left-to-right scanning of the input,
- `R`: constructing a rightmost derivations in reverse,
- `k`: the number of input symbols of lookahead that are used in making parsing decisions.

### 4.6.1 Why LR Parsers?
### 4.6.2 Items and the LR(0) Automaton
### 4.6.3 The LR-Parsing Algorithm
### 4.6.4 Constructing SLR-Parsing Tables
### 4.6.5 Viable Prefixes
### 4.6.6 Exercises for Section 4.6

## 4.7 More Powerful LR Parsers
### 4.7.1 Canonical LR(1) Items
### 4.7.2 Constructing LR(1) Sets of Items
### 4.7.3 Canonical LR(1) Parsing Tables
### 4.7.4 Constructing LALR Parsing Tables
### 4.7.5 Ecient Construction of LALR Parsing Tables
### 4.7.6 Compaction of LR Parsing Tables
### 4.7.7 Exercises for Section 4.7

## 4.8 Using Ambiguous Grammars
### 4.8.1 Precedence and Associativity to Resolve Conflicts
### 4.8.2 The "Dangling-Else" Ambiguity
### 4.8.3 Error Recovery in LR Parsing
### 4.8.4 Exercises for Section 4.8

## 4.9 Parser Generators
### 4.9.1 The Parser Generator Yacc
### 4.9.2 Using Yacc with Ambiguous Grammars
### 4.9.3 Creating Yacc Lexical Analyzers with Lex
### 4.9.4 Error Recovery in Yacc
### 4.9.5 Exercises for Section 4.9

## 4.10 Summary of Chapter 4
## 4.11 References for Chapter 4

- 1 Aho, A. V., S. C. Johnson, and J. D. Ullman, **Deterministic parsing of ambiguous grammars** Comm. ACM 18:8 (Aug., 1975), pp. 441-452.
- 2 Backus, J.W, **The syntax and semantics of the proposed international algebraic language of the Zurich-ACM-GAMM Conference**, Proc. Intl. Conf. Information Processing, UNESCO, Paris, (1959) pp. 125-132.
- 3 Birman, A. and J. D. Ullman, **Parsing algorithms with backtrack**, Information and Control 23:1 (1973), pp. 1-34.
- 4 Cantor, D. C., **On the ambiguity problem of Backus systems**, J. ACM 9:4 (1962), pp. 477-479.
- 5 Chomsky, N., **Three models for the description of language**, IRE Trans. on Information Theory IT-2:3 (1956), pp. 113-124.
- 6 Chomsky, N., **On certain formal properties of grammars**, Information and Control 2:2 (1959), pp. 137-167.
- 7 Dain, J., **Bibliography on Syntax Error Handling in Language Translation Systems**, 1991. Available from the comp.compilers newsgroup; see http://compilers.iecc.com/comparch/article/91-04-050 .
- 8 DeRemer, F., **Practical Translators for LR(k) Languages**, Ph.D. thesis, MIT, Cambridge, MA, 1969.
- 9 DeRemer, F., **Simple LR(k) grammars**, Comm. ACM 14:7 (July, 1971), pp. 453-460.
- 10 Donnelly, C. and R. Stallman, **Bison: The YACC-compatible Parser Generator**, http://www.gnu.org/software/bison/manual/ .
- 11 Earley, J., **An ecient context-free parsing algorithm**, Comm. ACM 13:2 (Feb., 1970), pp. 94-102.
- 12 Earley, J., **Ambiguity and precedence in syntax description**, Acta Informatica 4:2 (1975), pp. 183-192.
- 13 Floyd, R. W., **On ambiguity in phrase-structure languages**, Comm. ACM 5:10 (Oct., 1962), pp. 526-534.
- 14 Floyd, R. W., **Syntactic analysis and operator precedence**, J. ACM 10:3 (1963), pp. 316-333.
